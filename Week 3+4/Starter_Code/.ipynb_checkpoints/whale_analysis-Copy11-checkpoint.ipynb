{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #  A Whale off the Port(folio)\n",
    " ---\n",
    "\n",
    " In this assignment, you'll get to use what you've learned this week to evaluate the performance among various algorithmic, hedge, and mutual fund portfolios and compare them against the S&P TSX 60 Index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions and limitations\n",
    "\n",
    "1. Limitation: Only dates that overlap between portfolios will be compared\n",
    "2. Assumption: There are no significant anomalous price impacting events during the time window such as share split, trading halt\n",
    "3. Assumption: S&P TSX 60 is representative of the market as a whole, acting as an index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports\n",
    "import pandas as pd # daataframe manipulation\n",
    "import numpy as np # calc and numeric manipulatino\n",
    "import datetime as dt # date and tim \n",
    "from pathlib import Path # setting the path for file manipulation\n",
    "import datetime\n",
    "import seaborn as sns # advanced plotting/charting library\n",
    "import matplotlib as plt\n",
    "pd.options.display.float_format = '{:.6f}'.format # float format to 6 decimal places"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "In this section, you will need to read the CSV files into DataFrames and perform any necessary data cleaning steps. After cleaning, combine all DataFrames into a single DataFrame.\n",
    "\n",
    "Files:\n",
    "\n",
    "* `whale_returns.csv`: Contains returns of some famous \"whale\" investors' portfolios.\n",
    "\n",
    "* `algo_returns.csv`: Contains returns from the in-house trading algorithms from Harold's company.\n",
    "\n",
    "* `sp_tsx_history.csv`: Contains historical closing prices of the S&P TSX 60 Index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Whale Returns\n",
    "\n",
    "Read the Whale Portfolio daily returns and clean the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. import whale csv and set index to date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wr = pd.read_csv('Resources/whale_returns.csv', index_col=\"Date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Inspect imported data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at colums and value head\n",
    "df_wr.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at last few values\n",
    "df_wr.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dimensions of df\n",
    "df_wr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get index datatype - for later merging\n",
    "df_wr.index.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get datatypes of all values\n",
    "df_wr.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Count and drop any null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count nulls\n",
    "df_wr.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop nulls \n",
    "df_wr.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count nulls -again to ensure they're removed\n",
    "df_wr.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wr.count() #double check all values are equal in length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Sort the index to ensure the correct date order for calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wr.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Rename columns - shorten and make consistent with other tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change columns to be consistent and informative\n",
    "df_wr.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wr.columns = ['Whale_Soros_Fund_Daily_Returns', 'Whale_Paulson_Daily_Returns',\n",
    "       'Whale_Tiger_Daily_Returns', 'Whale_Berekshire_Daily_Returns']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Create copy dataframe with new column for cumulative returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the dataframe to store cumprod in a new view\n",
    "df_wr_cumulative = df_wr.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column in new df for each cumulative daily return using the cumprod function\n",
    "df_wr_cumulative['Whale_Soros_Fund_Daily_CumReturns'] = (1 + df_wr_cumulative['Whale_Soros_Fund_Daily_Returns']).cumprod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wr_cumulative['Whale_Paulson_Daily_CumReturns'] = (1 + df_wr_cumulative['Whale_Paulson_Daily_Returns']).cumprod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wr_cumulative['Whale_Tiger_Daily_CumReturns'] = (1 + df_wr_cumulative['Whale_Tiger_Daily_Returns']).cumprod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wr_cumulative['Whale_Berekshire_Daily_CumReturns'] = (1 + df_wr_cumulative['Whale_Berekshire_Daily_Returns']).cumprod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wr_cumulative.head() # check result is consistent against original column ie adds up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop returns columns from cumulative df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wr_cumulative.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wr_cumulative = df_wr_cumulative[['Whale_Soros_Fund_Daily_CumReturns', 'Whale_Paulson_Daily_CumReturns','Whale_Tiger_Daily_CumReturns', 'Whale_Berekshire_Daily_CumReturns']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wr_cumulative.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Look at high level stats & plot for whale portfolios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wr.describe(include='all') # basic stats for daily whale returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wr_cumulative.describe(include='all') # basic stats for daily cumulative whale returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot daily returns - whales\n",
    "df_wr.plot(figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cumulative returns\n",
    "df_wr_cumulative.plot(figsize=(10,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The data looks consistent and there are no obvious data errors identified. \n",
    "\n",
    "#### Initial high level observations of standalone daily returns data for whale portfolio:  At initial glance, the mean daily return indicates that Berkshire portfolio performed best (mean daily returns of 0.000501, mean cumulative daily returns 1.159732), while Paulson worst (-0.000203). The standard deviation indicates highest risk for Berkshire (0.012831 STD), while lowest risk/volatility is Paulson (std 0.006977)\n",
    "#### A more thorough analysis will be done in the following analysis section, so no conclusions are drawn yet. \n",
    "#### By looking at the cumulative chart, it is evident that all portfolios were vulnerable to a loss at the same tim around 2019-02-16, but that Berkshir was able to increas the most over time and climb the steepest after the downturn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Algorithmic Daily Returns\n",
    "\n",
    "Read the algorithmic daily returns and clean the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. import algo csv and set index to date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading algorithmic returns\n",
    "df_ar = pd.read_csv('Resources/algo_returns.csv', index_col='Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Inspect resulting dataframe and contained data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at colums and value first 3 rows\n",
    "df_ar.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at colums and value last 3 rows\n",
    "df_ar.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dimensions of df\n",
    "df_ar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get index datatype - for later merging\n",
    "df_ar.index.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get datatypes\n",
    "df_ar.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Count and remove null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count nulls\n",
    "df_ar.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop nulls\n",
    "df_ar.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count nulls -again to ensure that nulls actually are removed\n",
    "df_ar.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Sort index to ensure correct date order for calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Rename columns to be consistent with future merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar.columns = ['Algo1_Daily_Returns', 'Algo2_Daily_Returns']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Create new column in a copy df for cumulative returns per Algo daily return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a df copy to store cumulative data\n",
    "df_ar_cumulative = df_ar.copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use cumprod to get the daily cumulative returns for each of the algos 1 and 2\n",
    "df_ar_cumulative['Algo1_Daily_CumReturns'] = (1 + df_ar_cumulative['Algo1_Daily_Returns']).cumprod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar_cumulative['Algo2_Daily_CumReturns'] = (1 + df_ar_cumulative['Algo2_Daily_Returns']).cumprod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the result is consistent with the daily returns for first few columns\n",
    "df_ar_cumulative.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns that are not required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar_cumulative.columns # get the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar_cumulative = df_ar_cumulative[['Algo1_Daily_CumReturns','Algo2_Daily_CumReturns']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check result - first few lines\n",
    "df_ar_cumulative.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Look at high level stats & plot for algo portfolios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar.describe(include='all') # stats for daily returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar_cumulative.describe(include='all') # stats for daily cumulative returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot daily returns  - algos\n",
    "df_ar.plot(figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot daily cumulative returns  - algos\n",
    "df_ar_cumulative.plot(figsize=(10,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The data looks consistent and there are no obvious errors identified. \n",
    "\n",
    "#### Initial observations of standalone daily returns data for Algo 1 vs Algo 2:  mean daily return indicates that Algo 1 (mean daily return 0.000654) performs slightly better than Algo 2 (mean daily return 0.000341), which is alo evident in the cumulative daily returns plot. When looking at just daily returns, Algo 2 is more risky, but when looking at cumulative returns, Algo 1 is more risky (ie higher standard deviation). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. S&P TSX 60 Returns\n",
    "\n",
    "Read the S&P TSX 60 historic closing prices and create a new daily returns DataFrame from the data. \n",
    "Note: this contains daily closing and not returns - needs to be converted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import S&P csv daily closing price (not returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading S&P TSX 60 Closing Prices\n",
    "\n",
    "df_sr = pd.read_csv('Resources/sp_tsx_history.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Inspect columns of dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at colums and value head\n",
    "df_sr.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at tail values\n",
    "df_sr.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note from dataframe inspection: \n",
    "#### 1. date column was not immediated converted because it is in\n",
    "#### a different format to the other csv files and \n",
    "#### needs to bee converted to consistent format first\n",
    "#### 2. Close cannot be explicitly converted to float as it has\n",
    "#### dollar and commas. \n",
    "#### 3. A new column for returns will need to be created from \n",
    "#### return calculations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dimension of df\n",
    "df_sr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Data Types\n",
    "df_sr.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Convert the date into a consistent format with other tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sr['Date']= pd.to_datetime(df_sr['Date']).dt.strftime('%Y-%m-%d')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Convert the date data to index and check format and data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set date as index\n",
    "df_sr.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sr.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sr.index.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Check for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count nulls - none observed\n",
    "df_ar.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Convert daily closing price to float (from string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the Closing column to b float type\n",
    "df_sr['Close']= df_sr['Close'].str.replace('$','')\n",
    "df_sr['Close']= df_sr['Close'].str.replace(',','')\n",
    "df_sr['Close']= df_sr['Close'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Data Types\n",
    "df_sr.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test \n",
    "df_sr.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check null values\n",
    "df_sr.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sr.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Sort the index for calculations of returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort_index \n",
    "df_sr.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sr.head(2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_sr.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Calculate daily returns and store in new column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equation: $r=\\frac{{p_{t}} - {p_{t-1}}}{p_{t-1}}$\n",
    "\n",
    "The daily return is the (current closing price minus the previous day closing price) all divided by the previous day closing price. The initial value has no daily return as there is no prior period to compare it with. \n",
    "\n",
    "Here the calculation uses the python shift function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_sr['SnP_TSX_60_Returns'] = (df_sr['Close'] - df_sr['Close'].shift(1))/ df_sr['Close'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sr.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Cross check conversion to daily returns against alternative method - pct_change function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sr['SnP_TSX_60_Returns'] = df_sr['Close'].pct_change()\n",
    "df_sr.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methods cross check - looks good - continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for null - first row would have null\n",
    "df_sr.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop nulls - first row\n",
    "df_sr.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename `Close` Column to be specific to this portfolio.\n",
    "df_sr.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Drop original Closing column - not needed for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sr = df_sr[['SnP_TSX_60_Returns']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sr.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Create new column in a copy df for cumulative returns per daily return S&P TSX 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sr_cumulative = df_sr.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use cumprod to get the daily cumulative returns for each of the algos 1 and 2\n",
    "df_sr_cumulative['SnP_TSX_60_CumReturns'] = (1+df_sr_cumulative['SnP_TSX_60_Returns']).cumprod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visually check first 10 rows to ensure that results make sense\n",
    "df_sr_cumulative.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop daily returns column from cumulative df\n",
    "df_sr_cumulative = df_sr_cumulative[['SnP_TSX_60_CumReturns']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sr_cumulative.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Look at high level stats & plot for algo portfolios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sr.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sr_cumulative.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot daily returns  - S&P TSX 60\n",
    "df_sr.plot(figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot daily returns  - S&P TSX 60\n",
    "df_sr_cumulative.plot(figsize=(10,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Combine Whale, Algorithmic, and S&P TSX 60 Returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Merge daily returns dataframes from all portfolios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the `concat` function to combine the two DataFrames by matching indexes (or in this case `Month`)\n",
    "merged_analysis_df_tmp = pd.concat([df_wr, df_ar ], axis=\"columns\", join=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_analysis_df_tmp.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the `concat` function to combine the two DataFrames by matching indexes\n",
    "merged_daily_returns_df = pd.concat([merged_analysis_df_tmp, df_sr ], axis=\"columns\", join=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_daily_returns_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_daily_returns_df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_daily_returns_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conduct Quantitative Analysis\n",
    "\n",
    "In this section, you will calculate and visualize performance and risk metrics for the portfolios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Anlysis\n",
    "\n",
    "#### Calculate and Plot the daily returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Plot of daily returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot daily returns of all portfolios\n",
    "drp = merged_daily_returns_df.plot(figsize=(20,10), rot=45, title='Comparison of Daily Returns on Stock Portfolios')\n",
    "drp.set_xlabel(\"Daily Returns\")\n",
    "drp.set_ylabel(\"Date\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate and Plot cumulative returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Merge Cumulative Daily Returns\n",
    "\n",
    "Calculations were already done in the first section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the `concat` function to combine the two DataFrames by matching indexes\n",
    "merged_cumulative__df_tmp = pd.concat([df_wr_cumulative, df_ar_cumulative ], axis=\"columns\", join=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_daily_cumreturns_df = pd.concat([merged_cumulative__df_tmp, df_sr_cumulative ], axis=\"columns\", join=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_daily_cumreturns_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cumulative returns\n",
    "\n",
    "dcrp = merged_daily_cumreturns_df.plot(figsize=(20,10), rot=45, title='Comparison of Daily Cumulative Returns on Stock Portfolios')\n",
    "dcrp.set_xlabel(\"Daily Cumulative Returns\")\n",
    "dcrp.set_ylabel(\"Date\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk Analysis\n",
    "\n",
    "Determine the _risk_ of each portfolio:\n",
    "\n",
    "1. Create a box plot for each portfolio. \n",
    "2. Calculate the standard deviation for all portfolios.\n",
    "4. Determine which portfolios are riskier than the S&P TSX 60.\n",
    "5. Calculate the Annualized Standard Deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a box plot for each portfolio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [[TODO - is this over the daily returns or cumulative returns??]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot to visually show risk\n",
    "mcrb = merged_daily_returns_df.plot.box(figsize=(20,10), rot=45, title='Boxplot Comparison of Daily Returns on Stock Portfolios')\n",
    "dcrp.set_xlabel(\"Daily Returns\")\n",
    "dcrp.set_ylabel(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Standard Deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily standard deviation of daily returns sorted in ascending ordeer\n",
    "daily_std = merged_daily_returns_df.std().sort_values()\n",
    "daily_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcrb = daily_std.plot.hist(figsize=(20,10), rot=45, title='Comparison of Standard Deviation of Daily Returns on Stock Portfolios')\n",
    "mcrb.set_xlabel(\"Returns Standard Deviation\")\n",
    "mcrb.set_ylabel(\"Portfolio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [[todo ]] annualized_std.plot.hist(stacked=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine which portfolios are riskier than the S&P TSX 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate  the daily standard deviation of S&P TSX 60\n",
    "\n",
    "# Determine which portfolios are riskier than the S&P TSX 60\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By sorting in ordere of srd deviation on daily return above, the riskier portfolios than S&P TSX 60 are all eexcept Whale Paulson portfolio, as all others have higher std deviation than S&P TSX 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the Annualized Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the annualized standard deviation (252 trading days)\n",
    "annualized_std = daily_std * np.sqrt(252)\n",
    "annualized_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling Statistics\n",
    "\n",
    "Risk changes over time. Analyze the rolling statistics for Risk and Beta. \n",
    "\n",
    "1. Calculate and plot the rolling standard deviation for the S&P TSX 60 using a 21-day window.\n",
    "2. Calculate the correlation between each stock to determine which portfolios may mimick the S&P TSX 60.\n",
    "3. Choose one portfolio, then calculate and plot the 60-day rolling beta for it and the S&P TSX 60."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate and plot rolling `std` for all portfolios with 21-day window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the rolling standard deviation for all portfolios using a 21-day window\n",
    "\n",
    "roll21_srd = merged_daily_returns_df.rolling(window=21).std()\n",
    "\n",
    "roll21_srd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the rolling standard deviation on all daily return (not closing price)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rollsp = merged_daily_returns_df.rolling(window=21).std().plot(figsize=(20,10), rot=45, title='21 Day Rolling Standard Deviation on Daily Returns on Stock Portfolios')\n",
    "rollsp.set_xlabel(\"21 Day Rolling Dates\")\n",
    "rollsp.set_ylabel(\"Standard Deviation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate and plot the correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation between each column\n",
    "correlation = merged_daily_returns_df.corr()\n",
    "correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display correlation matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.gcf()\n",
    "\n",
    "# Set the title\n",
    "plt.title('Inter-Portfolio Correlations')\n",
    "\n",
    "# Change seaborn plot size\n",
    "fig.set_size_inches(12, 8)\n",
    "\n",
    "\n",
    "sns.heatmap(correlation, vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate and Plot Beta for a chosen portfolio and the S&P 60 TSX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Covariance of Whales against SnP TSX 60 Returns\n",
    "Whale_Soros_Covariance = df_wr[\"Whale_Soros_Fund_Daily_Returns\"].cov(df_sr[\"SnP_TSX_60_Returns\"])\n",
    "Whale_Paulson_Covariance = df_wr[\"Whale_Paulson_Daily_Returns\"].cov(df_sr[\"SnP_TSX_60_Returns\"])\n",
    "Whale_Tiger_Covariance = df_wr[\"Whale_Tiger_Daily_Returns\"].cov(df_sr[\"SnP_TSX_60_Returns\"])\n",
    "Whale_Berekshire_Covariance = df_wr[\"Whale_Berekshire_Daily_Returns\"].cov(df_sr[\"SnP_TSX_60_Returns\"])\n",
    "\n",
    "# Display the covariance of each whale sub-portfolio\n",
    "print(\"Soros Covariance: \", \"%.16f\" % Whale_Soros_Covariance)\n",
    "print(\"Paulson Covariance: \", \"%.16f\" % Whale_Paulson_Covariance)\n",
    "print(\"Tiger Covariance: \", \"%.16f\" % Whale_Tiger_Covariance)\n",
    "print(\"Berekshire Covariance: \", \"%.16f\" % Whale_Berekshire_Covariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covariance of Whales against SnP TSX 60 Returns\n",
    "Algo1_Covariance = df_ar[\"Algo1_Daily_Returns\"].cov(df_sr[\"SnP_TSX_60_Returns\"])\n",
    "Algo2_Covariance = df_ar[\"Algo2_Daily_Returns\"].cov(df_sr[\"SnP_TSX_60_Returns\"])\n",
    "\n",
    "# Display the covariance of each whale sub-portfolio\n",
    "print(\"Algo1 Covariance: \", \"%.16f\" % Algo1_Covariance)\n",
    "print(\"Algo2 Covariance: \", \"%.16f\" % Algo2_Covariance)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covariance of algos portfolio (within the portfolio)\n",
    "covariance_algo = df_ar.cov()\n",
    "covariance_algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covariance of s&p 60 TSR portfolio\n",
    "covariance_snp = df_sr.cov()\n",
    "covariance_snp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate covariance of a single sub-portfolio streams in portfolios\n",
    "# how each individual sub-portfolios covary with other sub-portfolios\n",
    "# similar evaluation to correlation heat map\n",
    "covariance_a = merged_daily_returns_df.cov()\n",
    "covariance_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate variance of S&P TSX\n",
    "variance_snp = df_sr.var()\n",
    "variance_snp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beta Values for Whales Sub-Portfolios\n",
    "# Calculate beta of all daily returns of whale portfolio\n",
    "Soros_beta = Whale_Soros_Covariance / variance_snp\n",
    "Paulson_beta = Whale_Paulson_Covariance / variance_snp\n",
    "Tiger_beta = Whale_Tiger_Covariance / variance_snp\n",
    "Berekshire_beta = Whale_Berekshire_Covariance / variance_snp\n",
    "\n",
    "\n",
    "# Display the covariance of each Whale sub-portfolio\n",
    "print(\"Soros Beta: \", \"%.16f\" % Soros_beta)\n",
    "print(\"Paulson Beta: \", \"%.16f\" % Paulson_beta)\n",
    "print(\"Tiger Beta: \", \"%.16f\" % Tiger_beta)\n",
    "print(\"Berekshire Beta: \", \"%.16f\" % Berekshire_beta)\n",
    "print(\"--------------------\")\n",
    "\n",
    "Average_Whale_beta = (Soros_beta  + Paulson_beta + Tiger_beta +  Berekshire_beta)/4\n",
    "print(\"Average Whale Beta: \", \"%.16f\" % Average_Whale_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beta Values for Algos Sub-Portfolios\n",
    "# Calculate beta of all daily returns of Algos portfolio\n",
    "Algo1_beta = Algo1_Covariance / variance_snp\n",
    "Algo2_beta = Algo2_Covariance / variance_snp\n",
    "\n",
    "\n",
    "# Display the covariance of each Algos sub-portfolio\n",
    "print(\"Algo1 Beta: \", \"%.16f\" % Algo1_beta)\n",
    "print(\"Algo2 Beta: \", \"%.16f\" % Algo2_beta)\n",
    "\n",
    "print(\"--------------------\")\n",
    "\n",
    "Average_Algo_beta = (Algo1_beta  + Algo2_beta)/2\n",
    "print(\"Average Algo Beta: \", \"%.16f\" % Average_Algo_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 21 day rolling covariance of algo portfolio stocks vs. S&P TSX 60\n",
    "rolling_algo1_covariance = merged_daily_returns_df[\"Algo1_Daily_Returns\"].rolling(window=21,  min_periods=1).cov(df_sr[\"SnP_TSX_60_Returns\"])\n",
    "rolling_algo2_covariance = merged_daily_returns_df[\"Algo2_Daily_Returns\"].rolling(window=21,  min_periods=1).cov(df_sr[\"SnP_TSX_60_Returns\"])\n",
    "\n",
    "# 21 day rolling covariance of whale portfolio stocks vs. S&P TSX 60\n",
    "rolling_Soros_covariance = merged_daily_returns_df[\"Whale_Soros_Fund_Daily_Returns\"].rolling(window=21,  min_periods=1).cov(df_sr[\"SnP_TSX_60_Returns\"])\n",
    "rolling_Paulson_covariance =  merged_daily_returns_df[\"Whale_Paulson_Daily_Returns\"].rolling(window=21,  min_periods=1).cov(df_sr[\"SnP_TSX_60_Returns\"])\n",
    "rolling_Tiger_covariance = merged_daily_returns_df[\"Whale_Tiger_Daily_Returns\"].rolling(window=21,  min_periods=1).cov(df_sr[\"SnP_TSX_60_Returns\"])\n",
    "rolling_Berkshire_covariance = merged_daily_returns_df[\"Whale_Berekshire_Daily_Returns\"].rolling(window=21,  min_periods=1).cov(df_sr[\"SnP_TSX_60_Returns\"])\n",
    "\n",
    "# 21 day rolling S&P TSX 60 covariance\n",
    "rolling_SnP_covariance = merged_daily_returns_df[\"SnP_TSX_60_Returns\"].rolling(window=21,  min_periods=1).cov(df_sr[\"SnP_TSX_60_Returns\"])\n",
    "\n",
    "\n",
    "# 21 day rolling variance of S&P TSX 60\n",
    "rolling_variance = merged_daily_returns_df[\"SnP_TSX_60_Returns\"].rolling(window=21).var()\n",
    "\n",
    "# 21 day rolling beta of algo portfolio stocks vs. S&P TSX 60\n",
    "rolling_algo1_beta = rolling_algo1_covariance / rolling_variance\n",
    "rolling_algo2_beta = rolling_algo2_covariance / rolling_variance\n",
    "\n",
    "# 21 day average beta for algo portfolio\n",
    "rolling_average_algo_beta = (rolling_algo1_beta + rolling_algo1_beta)/2\n",
    "\n",
    "# 21 day rolling beta of whale portfolio stocks vs. S&P TSX 60\n",
    "rolling_Soros_beta = rolling_Soros_covariance / rolling_variance\n",
    "rolling_Paulson_beta = rolling_Paulson_covariance / rolling_variance\n",
    "rolling_Tiger_beta = rolling_Tiger_covariance / rolling_variance\n",
    "rolling_Berkshire_beta = rolling_Berkshire_covariance / rolling_variance\n",
    "rolling_SnP_Beta = rolling_SnP_covariance/ rolling_variance\n",
    "\n",
    "# 21 day average beta for whale portfolio\n",
    "rolling_average_whale_beta = (rolling_Soros_beta + rolling_Paulson_beta + rolling_Tiger_beta + rolling_Berkshire_beta)/4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the figure and plot the different social media beta values as multiple trends on the same figure\n",
    "ax = rolling_algo1_covariance.plot(figsize=(20, 10), title=\"Rolling 21 Day Covariance of Sub-Portfolio Returns vs. S&P TSX 60 Returns\")\n",
    "rolling_algo2_covariance.plot(ax=ax)\n",
    "rolling_Soros_covariance.plot(ax=ax)\n",
    "rolling_Paulson_covariance.plot(ax=ax)\n",
    "rolling_Tiger_covariance.plot(ax=ax)\n",
    "rolling_Berkshire_covariance.plot(ax=ax)\n",
    "\n",
    "# Set the legend of the figure\n",
    "ax.legend([\"Algo1 Covariance\", \"Algo2 Covariance\", \"Whale Soros Covariance\", \"Whale Paulson Covariance\", \"Whale Tiger Covariance\",\"Whale Berkshire Covariance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rolling_algo1_covariance.plot(figsize=(20, 10), title='Rolling 21 Day Covariance of Sub-Portfolio Returns vs. S&P TSX 60 Returns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_algo1_beta.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Plot beta trend\n",
    "# Set the figure and plot the different social media beta values as multiple trends on the same figure\n",
    "ax = rolling_algo1_beta.plot(figsize=(20, 10), title=\"Rolling 21 Day Beta of Sub-Portfolio Returns vs. S&P TSX 60 Returns\")\n",
    "rolling_algo2_beta.plot(ax=ax)\n",
    "rolling_Soros_beta.plot(ax=ax)\n",
    "rolling_Paulson_beta.plot(ax=ax)\n",
    "rolling_Tiger_beta.plot(ax=ax)\n",
    "rolling_Berkshire_beta.plot(ax=ax)\n",
    "rolling_SnP_Beta.plot(ax=ax)\n",
    "\n",
    "# Set the legend of the figure\n",
    "ax.legend([\"Algo1 Beta\", \"Algo2 Beta\", \"Whale Soros Beta\", \"Whale Paulson Beta\", \"Whale Tiger Beta\",\"Whale Berkshire Beta\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling Statistics Challenge: Exponentially Weighted Average \n",
    "\n",
    "An alternative way to calculate a rolling window is to take the exponentially weighted moving average. This is like a moving window average, but it assigns greater importance to more recent observations. Try calculating the [`ewm`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.ewm.html) with a 21-day half-life."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use `ewm` to calculate the rolling window\n",
    "ewm_21_algo1 = rolling_algo1_beta.ewm(halflife=21, adjust=False).mean()\n",
    "ewm_21_algo2 = rolling_algo2_beta.ewm(halflife=21, adjust=False).mean()\n",
    "ewm_21_soros = rolling_Soros_beta.ewm(halflife=21, adjust=False).mean()\n",
    "ewm_21_paulson = rolling_Paulson_beta.ewm(halflife=21, adjust=False).mean()\n",
    "ewm_21_tiger = rolling_Tiger_beta.ewm(halflife=21, adjust=False).mean()\n",
    "ewm_21_berkshire = rolling_Berkshire_beta.ewm(halflife=21, adjust=False).mean()\n",
    "ewm_SnP_whale = rolling_SnP_Beta.ewm(halflife=21, adjust=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Plot beta trend\n",
    "# Set the figure and plot the different social media beta values as multiple trends on the same figure\n",
    "ax = ewm_21_algo1.plot(figsize=(20, 10), title=\"Rolling 21 Day Beta of Sub-Portfolio Returns vs. S&P TSX 60 Returns\")\n",
    "ewm_21_algo2.plot(ax=ax)\n",
    "ewm_21_soros.plot(ax=ax)\n",
    "ewm_21_paulson.plot(ax=ax)\n",
    "ewm_21_tiger.plot(ax=ax)\n",
    "ewm_21_berkshire.plot(ax=ax)\n",
    "ewm_SnP_whale.plot(ax=ax)\n",
    "\n",
    "# Set the legend of the figure\n",
    "ax.legend([\"Algo1 EWM Beta\", \"Algo2 EWM Beta\", \"Whale EWM Soros Beta\", \"Whale EWM Paulson Beta\", \"Whale EWM Tiger Beta\",\"Whale EWM Berkshire Beta\",\"Whale EWM S&P Beta\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ewm_21_whale.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ewm_21_algo = merged_daily_returns_df.ewm(halflife=21, min_periods=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ewm_21_algo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'B': [0, 1, 2, np.nan, 4]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ewm(com=0.5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "df.ewm(halflife='4 days', times=pd.DatetimeIndex(times)).mean()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ewm_21_all = merged_daily_returns_df.ewm(halflife='21', times='index', min_periods=21, adjust=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ewm_21_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = pd.DataFrame(index=index, columns=columns)\n",
    "df_ = df_.fillna(0) \n",
    "df_ewm['rolling_Soros_beta'] = rolling_Soros_beta\n",
    "df_ewm['rolling_Paulson_beta'] = rolling_Paulson_beta\n",
    "df_ewm['rolling_Tiger_beta'] = rolling_Tiger_beta\n",
    "df_ewm['rolling_Berkshire_beta'] = rolling_Berkshire_beta\n",
    "df_ewm['rolling_algo1_beta'] = rolling_algo1_beta\n",
    "df_ewm['rolling_algo2_beta'] = rolling_algo2_beta\n",
    "d_ewm_list.append(df_ewm['rolling_Soros_beta'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the rolling beta into a dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ewm.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ewm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ewm['Algo1_beta']\n",
    "\n",
    "rolling_algo2_beta.plot(ax=ax)\n",
    "rolling_Soros_beta.plot(ax=ax)\n",
    "rolling_Paulson_beta.plot(ax=ax)\n",
    "rolling_Tiger_beta.plot(ax=ax)\n",
    "rolling_Berkshire_beta.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ewm_21_all.plot(figsize=(20, 10), title=\"Rolling 21 Day Beta of Sub-Portfolio Returns vs. S&P TSX 60 Returns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/pandas-dev/pandas/issues/25147\n",
    "seed_ewm = df['B'].rolling(2).mean().iloc[1]\n",
    "\n",
    "df.iloc[:2] = seed_ewm\n",
    "\n",
    "df.ewm(span=2,min_periods=2,adjust=False).mean()\n",
    "Out[129]: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sharpe Ratios\n",
    "In reality, investment managers and thier institutional investors look at the ratio of return-to-risk, and not just returns alone. After all, if you could invest in one of two portfolios, and each offered the same 10% return, yet one offered lower risk, you'd take that one, right?\n",
    "\n",
    "### Using the daily returns, calculate and visualize the Sharpe ratios using a bar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annualized Sharpe Ratios\n",
    "sharpe_ratios = (merged_daily_returns_df.mean() * 252) / (merged_daily_returns_df.std() * np.sqrt(252))\n",
    "sharpe_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the sharpe ratios as a bar plot\n",
    "# Plot sharpe ratios\n",
    "sharpe_ratios.plot(kind=\"bar\", title=\"Sharpe Ratios\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [[TODO]] Get individual portfolio average sharp ratios to compare overall portfolio types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate standar deviaton for all investments for each portfolio\n",
    "harold_std_annual = harold_returns.std() * np.sqrt(252)\n",
    "my_std_annual = my_returns.std() * np.sqrt(252)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate standar deviaton for all investments for each portfolio\n",
    "harold_std_annual = harold_returns.std() * np.sqrt(252)\n",
    "my_std_annual = my_returns.std() * np.sqrt(252)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sharpe ratio\n",
    "harold_sharpe_ratios = (harold_returns.mean() * 252) / (harold_std_annual)\n",
    "my_sharpe_ratios = (my_returns.mean() * 252) / (my_std_annual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine whether the algorithmic strategies outperform both the market (S&P TSX 60) and the whales portfolios.\n",
    "\n",
    "Write your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Custom Portfolio\n",
    "\n",
    "In this section, you will build your own portfolio of stocks, calculate the returns, and compare the results to the Whale Portfolios and the S&P TSX 60. \n",
    "\n",
    "1. Choose 3-5 custom stocks with at last 1 year's worth of historic prices and create a DataFrame of the closing prices and dates for each stock.\n",
    "2. Calculate the weighted returns for the portfolio assuming an equal number of shares for each stock.\n",
    "3. Join your portfolio returns to the DataFrame that contains all of the portfolio returns.\n",
    "4. Re-run the performance and risk analysis with your portfolio to see how it compares to the others.\n",
    "5. Include correlation analysis to determine which stocks (if any) are correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose 3-5 custom stocks with at last 1 year's worth of historic prices and create a DataFrame of the closing prices and dates for each stock.\n",
    "\n",
    "For this demo solution, we fetch data from three companies listes in the S&P TSX 60 index.\n",
    "\n",
    "* `SHOP` - [Shopify Inc](https://en.wikipedia.org/wiki/Shopify)\n",
    "\n",
    "* `OTEX` - [Open Text Corporation](https://en.wikipedia.org/wiki/OpenText)\n",
    "\n",
    "* `L` - [Loblaw Companies Limited](https://en.wikipedia.org/wiki/Loblaw_Companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_daily_returns_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Get Daily Returns for Shopify Stocks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read in csv shopify data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data from 1st stock - shopify\n",
    "df_shop = pd.read_csv('Resources/Shopify.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shop.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shop.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shop.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shop.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Convert date to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shop['Date']= pd.to_datetime(df_shop['Date']).dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shop.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set date as index\n",
    "df_shop.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shop.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Remove unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shop.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shop = df_shop[['Close']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Sort date index ascending just in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shop.sort_index(inplace=True) # probably not necssary but just in case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Get daily returns & remove closing cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shop['Shop_Daily_Returns'] = df_shop['Close'].pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shop = df_shop[['Shop_Daily_Returns']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Review and drop  nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shop.isna().sum() # first row would be null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shop.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shop.isna().sum() #null should be gone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Get Daily Returns For Open Text Stocks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read in csv for Open Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data from 2nd stock - Otex\n",
    "df_otex = pd.read_csv('Resources/Otex.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Inspect dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Convert date to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_otex['Date']= pd.to_datetime(df_otex['Date']).dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set date as index\n",
    "df_otex.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4. Remove unwanted columns -declutter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_otex = df_otex[['Close']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5. Sort date index ascending just in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_otex.sort_index(inplace=True) # probably not necssary but just in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6. Get daily returns & remove closing cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_otex['Otex_Daily_Returns'] = df_otex['Close'].pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_otex = df_otex[['Otex_Daily_Returns']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7. Review and drop nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_otex.isna().sum() # first row would be null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_otex.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Get Returns for Loblaw Stocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read in csv for Loblaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data from 3rd stock - Loblaw\n",
    "df_lob = pd.read_csv('Resources/TSE_L.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Inspect dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Convert date to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lob['Date']= pd.to_datetime(df_lob['Date']).dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set date as index\n",
    "df_lob.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4. Remove unwanted columns -declutter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lob = df_lob[['Close']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5. Sort date index ascending just in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lob.sort_index(inplace=True) # probably not necssary but just in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6. Get daily returns & remove closing cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lob['Loblaw_Daily_Returns'] = df_lob['Close'].pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lob = df_lob[['Loblaw_Daily_Returns']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7. Review and drop nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lob.isna().sum() # first row would be null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lob.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all stocks in a single DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset Date index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorganize portfolio data by having a column per symbol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate daily returns\n",
    "\n",
    "# Drop NAs\n",
    "\n",
    "# Display sample data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the weighted returns for the portfolio assuming an equal number of shares for each stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set weights\n",
    "weights = [1/3, 1/3, 1/3]\n",
    "\n",
    "# Calculate portfolio return\n",
    "\n",
    "# Display sample data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join your portfolio returns to the DataFrame that contains all of the portfolio returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join your returns DataFrame to the original returns DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only compare dates where return data exists for all the stocks (drop NaNs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-run the risk analysis with your portfolio to see how it compares to the others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the Annualized Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the annualized `std`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate and plot rolling `std` with 21-day window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate rolling standard deviation\n",
    "\n",
    "# Plot rolling standard deviation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate and plot the correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and plot the correlation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate and Plot the 60-day Rolling Beta for Your Portfolio compared to the S&P 60 TSX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and plot Beta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the daily returns, calculate and visualize the Sharpe ratios using a bar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Annualzied Sharpe Ratios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the sharpe ratios as a bar plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does your portfolio do?\n",
    "\n",
    "Write your answer here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "Shift function in pandas - \n",
    "https://stackoverflow.com/questions/20000726/calculate-daily-returns-with-pandas-dataframe\n",
    "\n",
    "Conditional line color -  \n",
    "https://stackoverflow.com/questions/31590184/plot-multicolored-line-based-on-conditional-in-python\n",
    "\n",
    "https://stackoverflow.com/questions/40803570/python-matplotlib-scatter-plot-specify-color-points-depending-on-conditions/40804861\n",
    "\n",
    "https://stackoverflow.com/questions/42453649/conditional-color-with-matplotlib-scatter\n",
    "\n",
    "https://stackoverflow.com/questions/3832809/how-to-change-the-color-of-a-single-bar-if-condition-is-true-matplotlib\n",
    "\n",
    "https://stackoverflow.com/questions/56779975/conditional-coloring-in-matplotlib-using-numpys-where\n",
    "\n",
    "Google finance - https://support.google.com/docs/answer/3093281?hl=en\n",
    "\n",
    "Boxplots - https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51\n",
    "\n",
    "PEP 8 - Standards - \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 4 Homework Assignment: A Whale Off the Port(folio)\n",
    "\n",
    "![Portfolio Analysis](Images/portfolio-analysis.png)\n",
    "\n",
    "## Background\n",
    "\n",
    "Harold's company has been investing in algorithmic trading strategies. Some of the investment managers love them, some hate them, but they all think their way is best.\n",
    "\n",
    "You just learned these quantitative analysis techniques with Python and Pandas, so Harold has come to you with a challengeto help him determine which portfolio is performing the best across multiple areas: volatility, returns, risk, and Sharpe ratios.\n",
    "\n",
    "You need to create a tool (an analysis notebook) that analyzes and visualizes the major metrics of the portfolios across all of these areas, and determine which portfolio outperformed the others. You will be given the historical daily returns of several portfolios: some from the firm's algorithmic portfolios, some that represent the portfolios of famous \"whale\" investors like Warren Buffett, and some from the big hedge and mutual funds. You will then use this analysis to create a custom portfolio of stocks and compare its performance to that of the other portfolios, as well as the larger market ([S&P TSX 60 Index](https://en.wikipedia.org/wiki/S%26P/TSX_60)).\n",
    "\n",
    "For this homework assignment, you have three main tasks:\n",
    "\n",
    "1. [Read in and Wrangle Returns Data](#Prepare-the-Data)\n",
    "\n",
    "2. [Determine Success of Each Portfolio](#Conduct-Quantitative-Analysis)\n",
    "\n",
    "3. [Choose and Evaluate a Custom Portfolio](#Create-a-Custom-Portfolio)\n",
    "\n",
    "---\n",
    "\n",
    "## Instructions\n",
    "\n",
    "**Files:**\n",
    "\n",
    "* [Whale Analysis Starter Code](Starter_Code/whale_analysis.ipynb)\n",
    "\n",
    "* [algo_returns.csv](Starter_Code/Resources/algo_returns.csv)\n",
    "\n",
    "* [otex_historical.csv](Starter_Code/Resources/otex_historical.csv)\n",
    "\n",
    "* [sp_tsx_history.csv](Starter_Code/Resources/sp_tsx_history.csv)\n",
    "\n",
    "* [l_historical.csv](Starter_Code/Resources/l_historical.csv)\n",
    "\n",
    "* [shop_historical.csv](Starter_Code/Resources/shop_historical.csv)\n",
    "\n",
    "* [whale_returns.csv](Starter_Code/Resources/whale_returns.csv)\n",
    "\n",
    "### Prepare the Data\n",
    "\n",
    "First, read and clean several CSV files for analysis. The CSV files include whale portfolio returns, algorithmic trading portfolio returns, and S&P TSX 60 Index historical prices. Use the starter code to complete the following steps:\n",
    "\n",
    "1. Use Pandas to read the following CSV files as a DataFrame. Be sure to convert the dates to a `DateTimeIndex`.\n",
    "\n",
    "    * `whale_returns.csv`: Contains returns of some famous \"whale\" investors' portfolios.\n",
    "\n",
    "    * `algo_returns.csv`: Contains returns from the in-house trading algorithms from Harold's company.\n",
    "\n",
    "    * `sp_tsx_history.csv`: Contains historical closing prices of the S&P TSX 60 Index.\n",
    "\n",
    "2. Detect and remove null values.\n",
    "\n",
    "3. If any columns have dollar signs or characters other than numeric values, remove those characters and convert the data types as needed.\n",
    "\n",
    "4. The whale portfolios and algorithmic portfolio CSV files contain daily returns, but the S&P TSX 60 CSV file contains closing prices. Convert the S&P TSX 60 closing prices to daily returns.\n",
    "\n",
    "5. Join `Whale Returns`, `Algorithmic Returns`, and the `S&P TSX 60 Returns` into a single DataFrame with columns for each portfolio's returns.\n",
    "\n",
    "    ![returns-dataframe.png](Images/returns-dataframe.png)\n",
    "\n",
    "### Conduct Quantitative Analysis\n",
    "\n",
    "Analyze the data to see if any of the portfolios outperform the stock market (i.e., the S&P TSX 60).\n",
    "\n",
    "#### Performance Analysis\n",
    "\n",
    "1. Calculate and plot daily returns of all portfolios.\n",
    "\n",
    "2. Calculate and plot cumulative returns for all portfolios. Does any portfolio outperform the S&P TSX 60?\n",
    "\n",
    "#### Risk Analysis\n",
    "\n",
    "1. Create a box plot for each of the returns. \n",
    "\n",
    "2. Calculate the standard deviation for each portfolio. \n",
    "\n",
    "3. Determine which portfolios are riskier than the S&P TSX 60\n",
    "\n",
    "4. Calculate the Annualized Standard Deviation.\n",
    "\n",
    "#### Rolling Statistics\n",
    "\n",
    "1. Calculate and plot the rolling standard deviation for all portfolios using a 21-day window.\n",
    "\n",
    "2. Calculate and plot the correlation between each stock to determine which portfolios may mimick the S&P TSX 60.\n",
    "\n",
    "3. Choose one portfolio, then calculate and plot the 60-day rolling beta for it and the S&P TSX 60.\n",
    "\n",
    "#### Rolling Statistics Challenge: Exponentially Weighted Average\n",
    "\n",
    "An alternative method to calculate a rolling window is to take the exponentially weighted moving average. This is like a moving window average, but it assigns greater importance to more recent observations. Try calculating the [`ewm`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.ewm.html) with a 21-day half-life.\n",
    "\n",
    "### Sharpe Ratios\n",
    "\n",
    "Investment managers and their institutional investors look at the return-to-risk ratio, not just the returns. After all, if you have two portfolios that each offer a 10% return, yet one is lower risk, you would invest in the lower-risk portfolio, right?\n",
    "\n",
    "1. Using the daily returns, calculate and visualize the Sharpe ratios using a bar plot.\n",
    "\n",
    "2. Determine whether the algorithmic strategies outperform both the market (S&P TSX 60) and the whales portfolios.\n",
    "\n",
    "### Create a Custom Portfolio\n",
    "\n",
    "Harold is ecstatic that you were able to help him prove that the algorithmic trading portfolios are doing so well compared to the market and whales portfolios. However, now you are wondering whether you can choose your own portfolio that performs just as well as the algorithmic portfolios. Investigate by doing the following:\n",
    "\n",
    "1. Visit [Google Sheets](https://docs.google.com/spreadsheets/) and use the built-in Google Finance function to choose 3-5 stocks for your portfolio.\n",
    "\n",
    "2. Download the data as CSV files and calculate the portfolio returns.\n",
    "\n",
    "3. Calculate the weighted returns for your portfolio, assuming equal number of shares per stock.\n",
    "\n",
    "4. Add your portfolio returns to the DataFrame with the other portfolios.\n",
    "\n",
    "5. Run the following analyses:\n",
    "\n",
    "    * Calculate the Annualized Standard Deviation.\n",
    "    * Calculate and plot rolling `std` with a 21-day window.\n",
    "    * Calculate and plot the correlation.\n",
    "    * Calculate and plot the 60-day rolling beta for your portfolio compared to the S&P 60 TSX.\n",
    "    * Calculate the Sharpe ratios and generate a bar plot.\n",
    "\n",
    "4. How does your portfolio do?\n",
    "\n",
    "---\n",
    "\n",
    "## Resources\n",
    "\n",
    "* [Pandas API Docs](https://pandas.pydata.org/pandas-docs/stable/reference/index.html)\n",
    "\n",
    "* [Exponential weighted function in Pandas](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.ewm.html)\n",
    "\n",
    "* [`GOOGLEFINANCE` function help](https://support.google.com/docs/answer/3093281)\n",
    "\n",
    "* [Supplemental Guide: Fetching Stock Data Using Google Sheets](../../../01-Lesson-Plans/04-Pandas/Supplemental/googlefinance_guide.md)\n",
    "\n",
    "---\n",
    "\n",
    "## Hints\n",
    "\n",
    "* After reading each CSV file, don't forget to sort each DataFrame in ascending order by the Date using `sort_index`. This is especially important when working with time series data, as we want to make sure Date indexes go from earliest to latest.\n",
    "\n",
    "* The Pandas functions used in class this week will be useful for this assignment.\n",
    "\n",
    "* Be sure to use `head()` or `tail()` when you want to look at your data, but don't want to print to a large DataFrame.\n",
    "\n",
    "---\n",
    "\n",
    "## Submission\n",
    "\n",
    "1. Use the provided starter Jupyter Notebook to house the code for your data preparation, analysis, and visualizations. Put any analysis or answers to assignment questions in raw text (markdown) cells in the report.\n",
    "\n",
    "2. Submit your notebook to a new GitHub repository.\n",
    "\n",
    "3. Add the URL of your GitHub repository to your assignment when submitting via Bootcamp Spot.\n",
    "\n",
    "---\n",
    "\n",
    " 2020 Trilogy Education Services, a 2U, Inc. brand. All Rights Reserved.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
